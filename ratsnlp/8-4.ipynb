{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC5OwyKMx_l9"
      },
      "source": [
        "# 각종 설정\n",
        "모델 하이퍼파라메터(hyperparameter)와 저장 위치 등 설정 정보를 선언합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fKybDwDqFIX5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/youngwon/anaconda3/envs/nlp_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-04-16 17:00:45.579505: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-16 17:00:46.082230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downstream_model_checkpoint_fpath: /media/youngwon/Neo/NeoChoi/TIL/Pytorch-DL/ratsnlp/8-2_content/nlpbook/checkpoint-generation/epoch=0-val_loss=2.33.ckpt\n"
          ]
        }
      ],
      "source": [
        "from ratsnlp.nlpbook.generation import GenerationDeployArguments\n",
        "args = GenerationDeployArguments(\n",
        "    pretrained_model_name=\"skt/kogpt2-base-v2\",\n",
        "    downstream_model_dir=\"/media/youngwon/Neo/NeoChoi/TIL/Pytorch-DL/ratsnlp/8-2_content/nlpbook/checkpoint-generation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3mThtbxyNyO"
      },
      "source": [
        "# 모델 로딩\n",
        "파인튜닝을 마친 GPT2 모델과 토크나이저를 읽어 들입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aFV031RZFRgD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(51200, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "pretrained_model_config = GPT2Config.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        ")\n",
        "model = GPT2LMHeadModel(pretrained_model_config)\n",
        "fine_tuned_model_ckpt = torch.load(\n",
        "    args.downstream_model_checkpoint_fpath,\n",
        "    map_location=torch.device(\"cpu\"),\n",
        ")\n",
        "model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C3amlsjpFd9i"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    eos_token=\"</s>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVsdmThyV_p"
      },
      "source": [
        "# 인퍼런스 함수 선언\n",
        "인퍼런스 함수를 선언합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fnzR9NMtFiAz"
      },
      "outputs": [],
      "source": [
        "def inference_fn(\n",
        "        prompt,\n",
        "        min_length=10,\n",
        "        max_length=50,\n",
        "        top_p=0.92,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.5,\n",
        "        no_repeat_ngram_size=3,\n",
        "        temperature=0.9,\n",
        "):\n",
        "    try:\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_ids,\n",
        "                do_sample=True,\n",
        "                top_p=float(top_p),\n",
        "                top_k=int(top_k),\n",
        "                min_length=int(min_length),\n",
        "                max_length=int(max_length),\n",
        "                repetition_penalty=float(repetition_penalty),\n",
        "                no_repeat_ngram_size=int(no_repeat_ngram_size),\n",
        "                temperature=float(temperature),\n",
        "           )\n",
        "        generated_sentence = tokenizer.decode([el.item() for el in generated_ids[0]])\n",
        "    except:\n",
        "        generated_sentence = \"\"\"처리 중 오류가 발생했습니다. <br>\n",
        "            변수의 입력 범위를 확인하세요. <br><br> \n",
        "            min_length: 1 이상의 정수 <br>\n",
        "            max_length: 1 이상의 정수 <br>\n",
        "            top-p: 0 이상 1 이하의 실수 <br>\n",
        "            top-k: 1 이상의 정수 <br>\n",
        "            repetition_penalty: 1 이상의 실수 <br>\n",
        "            no_repeat_ngram_size: 1 이상의 정수 <br>\n",
        "            temperature: 0 이상의 실수\n",
        "            \"\"\"\n",
        "    return {\n",
        "        'result': generated_sentence,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6KshHb4P_0wj"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'result': '긍정 스토리는 진부하고 뻔한듯 하다. 하지만 내 취향인 영화.</s>'}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inference_fn(prompt = \"긍정 스토리는\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'result': '부정 스토리는 괜찮은데 전개가 이상하다~</s>'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inference_fn(prompt = \"부정 스토리는\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "generation-deploy-colab2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
